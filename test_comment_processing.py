#!/usr/bin/env python3
"""
è¯„è®ºè¯­ä¹‰å¤„ç†æ¨¡å—æµ‹è¯•è„šæœ¬
"""
import asyncio
import sys
import os
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from app.core.config import settings
from app.core.database import get_sync_session
from app.services.semantic_search_service import semantic_search_service
from app.services.comment_processing_service import comment_processing_service
from app.models.raw_comment_update import RawComment, ProcessingStatus
from app.models.comment_processing import ProductFeature, ProcessedComment
from app.core.logging import app_logger


def test_database_connection():
    """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
    print("ğŸ”— æµ‹è¯•æ•°æ®åº“è¿æ¥...")
    try:
        with get_sync_session() as session:
            # æµ‹è¯•æŸ¥è¯¢
            result = session.execute("SELECT 1 as test").fetchone()
            print(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ: {result}")
            return True
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return False


def test_product_features():
    """æµ‹è¯•äº§å“åŠŸèƒ½æ¨¡å—æ•°æ®"""
    print("\nğŸ“‹ æµ‹è¯•äº§å“åŠŸèƒ½æ¨¡å—æ•°æ®...")
    try:
        with get_sync_session() as session:
            # æŸ¥è¯¢äº§å“åŠŸèƒ½æ¨¡å—æ•°é‡
            count = session.query(ProductFeature).count()
            print(f"ğŸ“Š äº§å“åŠŸèƒ½æ¨¡å—æ€»æ•°: {count}")
            
            if count > 0:
                # æ˜¾ç¤ºå‰5ä¸ªåŠŸèƒ½æ¨¡å—
                features = session.query(ProductFeature).limit(5).all()
                print("ğŸ” å‰5ä¸ªåŠŸèƒ½æ¨¡å—:")
                for feature in features:
                    print(f"  - {feature.feature_code}: {feature.feature_name}")
                return True
            else:
                print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°äº§å“åŠŸèƒ½æ¨¡å—æ•°æ®ï¼Œè¯·å…ˆå¯¼å…¥æ•°æ®")
                return False
                
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢äº§å“åŠŸèƒ½æ¨¡å—å¤±è´¥: {e}")
        return False


def test_raw_comments():
    """æµ‹è¯•åŸå§‹è¯„è®ºæ•°æ®"""
    print("\nğŸ’¬ æµ‹è¯•åŸå§‹è¯„è®ºæ•°æ®...")
    try:
        with get_sync_session() as session:
            # æŸ¥è¯¢å¾…å¤„ç†è¯„è®ºæ•°é‡
            new_count = session.query(RawComment).filter(
                RawComment.processing_status == ProcessingStatus.NEW
            ).count()
            
            total_count = session.query(RawComment).count()
            
            print(f"ğŸ“Š åŸå§‹è¯„è®ºæ€»æ•°: {total_count}")
            print(f"ğŸ“Š å¾…å¤„ç†è¯„è®ºæ•°: {new_count}")
            
            if new_count > 0:
                # æ˜¾ç¤ºå‰3æ¡å¾…å¤„ç†è¯„è®º
                comments = session.query(RawComment).filter(
                    RawComment.processing_status == ProcessingStatus.NEW
                ).limit(3).all()
                
                print("ğŸ” å‰3æ¡å¾…å¤„ç†è¯„è®º:")
                for comment in comments:
                    content_preview = comment.comment_content[:100] + "..." if len(comment.comment_content) > 100 else comment.comment_content
                    print(f"  - ID {comment.raw_comment_id}: {content_preview}")
                return True
            else:
                print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¾…å¤„ç†çš„è¯„è®º")
                return False
                
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢åŸå§‹è¯„è®ºå¤±è´¥: {e}")
        return False


def test_semantic_search_service():
    """æµ‹è¯•è¯­ä¹‰æœç´¢æœåŠ¡"""
    print("\nğŸ” æµ‹è¯•è¯­ä¹‰æœç´¢æœåŠ¡...")
    try:
        # åˆå§‹åŒ–æœåŠ¡
        print("ğŸš€ åˆå§‹åŒ–è¯­ä¹‰æœç´¢æœåŠ¡...")
        semantic_search_service._initialize_embeddings()
        semantic_search_service._load_product_features()
        semantic_search_service._create_vector_store()
        
        # æµ‹è¯•æ–‡æœ¬åˆ†å—
        test_text = "è¿™ä¸ªè½¦çš„å‰ç¢°æ’é¢„è­¦ç³»ç»Ÿå¾ˆå¥½ç”¨ï¼Œåœ¨é«˜é€Ÿä¸Šèƒ½åŠæ—¶æé†’æˆ‘å‰æ–¹æœ‰éšœç¢ç‰©ã€‚è‡ªé€‚åº”å·¡èˆªæ§åˆ¶ä¹Ÿå¾ˆæ™ºèƒ½ï¼Œèƒ½å¤Ÿè‡ªåŠ¨è°ƒèŠ‚è½¦é€Ÿã€‚"
        chunks = semantic_search_service.split_text(test_text)
        print(f"ğŸ“ æµ‹è¯•æ–‡æœ¬åˆ†å—: {len(chunks)} ä¸ªå—")
        for i, chunk in enumerate(chunks):
            print(f"  å— {i+1}: {chunk}")
        
        # æµ‹è¯•è¯­ä¹‰æœç´¢
        if chunks:
            results = semantic_search_service.search_similar_features(chunks[0])
            print(f"ğŸ¯ è¯­ä¹‰æœç´¢ç»“æœ: {len(results)} ä¸ªåŒ¹é…")
            for result in results[:3]:  # æ˜¾ç¤ºå‰3ä¸ªç»“æœ
                print(f"  - {result['feature_code']}: {result['feature_name']} (ç›¸ä¼¼åº¦: {result['similarity_score']:.3f})")
        
        return True
        
    except Exception as e:
        print(f"âŒ è¯­ä¹‰æœç´¢æœåŠ¡æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_comment_processing():
    """æµ‹è¯•è¯„è®ºå¤„ç†æœåŠ¡"""
    print("\nâš™ï¸ æµ‹è¯•è¯„è®ºå¤„ç†æœåŠ¡...")
    try:
        # è·å–ä¸€æ¡å¾…å¤„ç†è¯„è®ºè¿›è¡Œæµ‹è¯•
        with get_sync_session() as session:
            comment = session.query(RawComment).filter(
                RawComment.processing_status == ProcessingStatus.NEW
            ).first()
            
            if not comment:
                print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¾…å¤„ç†çš„è¯„è®ºï¼Œè·³è¿‡å¤„ç†æµ‹è¯•")
                return True
            
            print(f"ğŸ¯ æµ‹è¯•å¤„ç†è¯„è®º ID: {comment.raw_comment_id}")
            print(f"ğŸ“ è¯„è®ºå†…å®¹: {comment.comment_content[:200]}...")
            
            # å¤„ç†å•æ¡è¯„è®º
            result = comment_processing_service.process_single_comment(comment.raw_comment_id)
            print(f"âœ… å¤„ç†ç»“æœ: {result}")
            
            return True
            
    except Exception as e:
        print(f"âŒ è¯„è®ºå¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_batch_processing():
    """æµ‹è¯•æ‰¹é‡å¤„ç†"""
    print("\nğŸ“¦ æµ‹è¯•æ‰¹é‡å¤„ç†...")
    try:
        # æ‰¹é‡å¤„ç†3æ¡è¯„è®º
        result = comment_processing_service.process_batch_comments(limit=3)
        print(f"âœ… æ‰¹é‡å¤„ç†ç»“æœ: {result}")
        return True
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_processing_statistics():
    """æµ‹è¯•å¤„ç†ç»Ÿè®¡"""
    print("\nğŸ“Š æµ‹è¯•å¤„ç†ç»Ÿè®¡...")
    try:
        stats = comment_processing_service.get_processing_statistics()
        print(f"âœ… å¤„ç†ç»Ÿè®¡: {stats}")
        return True
        
    except Exception as e:
        print(f"âŒ å¤„ç†ç»Ÿè®¡æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª è¯„è®ºè¯­ä¹‰å¤„ç†æ¨¡å—æµ‹è¯•å¼€å§‹")
    print("=" * 60)
    
    # æµ‹è¯•é…ç½®
    print(f"ğŸ”§ é…ç½®ä¿¡æ¯:")
    print(f"  - åµŒå…¥æ¨¡å‹API: {settings.EMBEDDING_API_BASE}")
    print(f"  - åµŒå…¥æ¨¡å‹åç§°: {settings.EMBEDDING_MODEL_NAME}")
    print(f"  - ç›¸ä¼¼åº¦é˜ˆå€¼: {settings.SEMANTIC_SIMILARITY_THRESHOLD}")
    
    tests = [
        ("æ•°æ®åº“è¿æ¥", test_database_connection),
        ("äº§å“åŠŸèƒ½æ¨¡å—", test_product_features),
        ("åŸå§‹è¯„è®ºæ•°æ®", test_raw_comments),
        ("è¯­ä¹‰æœç´¢æœåŠ¡", test_semantic_search_service),
        ("è¯„è®ºå¤„ç†æœåŠ¡", test_comment_processing),
        ("æ‰¹é‡å¤„ç†", test_batch_processing),
        ("å¤„ç†ç»Ÿè®¡", test_processing_statistics),
    ]
    
    passed = 0
    failed = 0
    
    for test_name, test_func in tests:
        try:
            print(f"\n{'='*20} {test_name} {'='*20}")
            if test_func():
                passed += 1
                print(f"âœ… {test_name} æµ‹è¯•é€šè¿‡")
            else:
                failed += 1
                print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥")
        except Exception as e:
            failed += 1
            print(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
    
    print("\n" + "=" * 60)
    print(f"ğŸ§ª æµ‹è¯•å®Œæˆ: é€šè¿‡ {passed}/{len(tests)}, å¤±è´¥ {failed}/{len(tests)}")
    
    if failed == 0:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼è¯„è®ºè¯­ä¹‰å¤„ç†æ¨¡å—å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œæ•°æ®ã€‚")


if __name__ == "__main__":
    main()