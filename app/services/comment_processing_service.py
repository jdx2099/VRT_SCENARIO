"""
è¯„è®ºå¤„ç†æœåŠ¡ - åŒæ­¥ç‰ˆæœ¬
ä¸“é—¨ç”¨äºCeleryä»»åŠ¡ï¼Œè´Ÿè´£è¯„è®ºçš„ç»“æ„åŒ–å¤„ç†å’Œå­˜å‚¨
"""
from typing import List, Dict, Optional
from sqlalchemy.orm import Session
from datetime import datetime, timezone

from app.core.database import get_sync_session
from app.core.logging import app_logger
from app.models.comment_processing import ProcessedComment
from app.models.raw_comment_update import RawComment, ProcessingStatus
from app.models.vehicle_update import ProcessingJob
from app.services.semantic_search_service import semantic_search_service


class CommentProcessingService:
    """
    è¯„è®ºå¤„ç†æœåŠ¡ç±» - åŒæ­¥ç‰ˆæœ¬
    ä¸“é—¨ç”¨äºCeleryä»»åŠ¡ï¼Œä½¿ç”¨pymysqlé©±åŠ¨
    """
    
    def __init__(self):
        self.logger = app_logger
    
    def save_processed_comments(self, processing_results: List[Dict], job_id: Optional[int] = None) -> int:
        """
        ä¿å­˜å¤„ç†ç»“æœåˆ°processed_commentsè¡¨
        
        Args:
            processing_results: å¤„ç†ç»“æœåˆ—è¡¨
            job_id: ä»»åŠ¡æ‰¹æ¬¡ID
            
        Returns:
            ä¿å­˜çš„è®°å½•æ•°é‡
        """
        if not processing_results:
            return 0
        
        try:
            with get_sync_session() as db:
                saved_count = 0
                
                for result in processing_results:
                    processed_comment = ProcessedComment(
                        raw_comment_id_fk=result["raw_comment_id"],
                        product_feature_id_fk=result["product_feature_id"],
                        feature_similarity_score=result["feature_similarity_score"],
                        job_id_fk=job_id,
                        comment_chunk_text=result["comment_chunk_text"],
                        comment_chunk_vector=result["comment_chunk_vector"],
                        feature_search_details=result["feature_search_details"],
                        processed_at=datetime.now(timezone.utc)
                    )
                    
                    db.add(processed_comment)
                    saved_count += 1
                
                db.commit()
                self.logger.info(f"âœ… æˆåŠŸä¿å­˜ {saved_count} æ¡å¤„ç†ç»“æœ")
                return saved_count
                
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜å¤„ç†ç»“æœå¤±è´¥: {e}")
            raise
    
    def process_single_comment(self, raw_comment: RawComment, job_id: Optional[int] = None) -> List[Dict]:
        """
        å¤„ç†å•æ¡è¯„è®º
        
        Args:
            raw_comment: åŸå§‹è¯„è®ºå¯¹è±¡
            job_id: ä»»åŠ¡æ‰¹æ¬¡ID
            
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        try:
            self.logger.info(f"ğŸ”„ å¼€å§‹å¤„ç†è¯„è®º: {raw_comment.raw_comment_id}")
            
            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
            semantic_search_service.update_comment_status(
                raw_comment.raw_comment_id, 
                ProcessingStatus.PROCESSING
            )
            
            # è¿›è¡Œè¯­ä¹‰æœç´¢å¤„ç†
            results = semantic_search_service.process_comment_chunks(
                raw_comment.raw_comment_id,
                raw_comment.comment_content
            )
            
            if results:
                # ä¿å­˜å¤„ç†ç»“æœ
                saved_count = self.save_processed_comments(results, job_id)
                
                # æ›´æ–°çŠ¶æ€ä¸ºå·²å®Œæˆ
                semantic_search_service.update_comment_status(
                    raw_comment.raw_comment_id,
                    ProcessingStatus.COMPLETED
                )
                
                self.logger.info(f"âœ… è¯„è®º {raw_comment.raw_comment_id} å¤„ç†å®Œæˆï¼Œä¿å­˜ {saved_count} æ¡ç»“æœ")
            else:
                # æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„åŠŸèƒ½æ¨¡å—ï¼Œæ ‡è®°ä¸ºè·³è¿‡
                semantic_search_service.update_comment_status(
                    raw_comment.raw_comment_id,
                    ProcessingStatus.SKIPPED
                )
                
                self.logger.info(f"âš ï¸ è¯„è®º {raw_comment.raw_comment_id} æœªæ‰¾åˆ°åŒ¹é…åŠŸèƒ½æ¨¡å—ï¼Œå·²è·³è¿‡")
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ å¤„ç†è¯„è®º {raw_comment.raw_comment_id} å¤±è´¥: {e}")
            
            # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
            try:
                semantic_search_service.update_comment_status(
                    raw_comment.raw_comment_id,
                    ProcessingStatus.FAILED
                )
            except:
                pass
            
            raise
    
    def process_batch_comments(self, limit: int = 20, job_id: Optional[int] = None) -> Dict:
        """
        æ‰¹é‡å¤„ç†è¯„è®º
        
        Args:
            limit: å¤„ç†æ•°é‡é™åˆ¶
            job_id: ä»»åŠ¡æ‰¹æ¬¡ID
            
        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡
        """
        try:
            self.logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†è¯„è®ºï¼Œé™åˆ¶æ•°é‡: {limit}")
            
            # è·å–å¾…å¤„ç†çš„è¯„è®º
            pending_comments = semantic_search_service.get_pending_comments(limit)
            
            if not pending_comments:
                self.logger.info("ğŸ“­ æ²¡æœ‰å¾…å¤„ç†çš„è¯„è®º")
                return {
                    "total_comments": 0,
                    "processed_count": 0,
                    "failed_count": 0,
                    "skipped_count": 0,
                    "total_results": 0
                }
            
            processed_count = 0
            failed_count = 0
            skipped_count = 0
            total_results = 0
            
            for comment in pending_comments:
                try:
                    results = self.process_single_comment(comment, job_id)
                    if results:
                        processed_count += 1
                        total_results += len(results)
                    else:
                        skipped_count += 1
                        
                except Exception as e:
                    self.logger.error(f"âŒ å¤„ç†è¯„è®º {comment.raw_comment_id} å¤±è´¥: {e}")
                    failed_count += 1
            
            summary = {
                "total_comments": len(pending_comments),
                "processed_count": processed_count,
                "failed_count": failed_count,
                "skipped_count": skipped_count,
                "total_results": total_results
            }
            
            self.logger.info(f"ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆ: {summary}")
            return summary
            
        except Exception as e:
            self.logger.error(f"âŒ æ‰¹é‡å¤„ç†è¯„è®ºå¤±è´¥: {e}")
            raise
    
    def get_processing_statistics(self) -> Dict:
        """
        è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            with get_sync_session() as db:
                # ç»Ÿè®¡å„çŠ¶æ€çš„è¯„è®ºæ•°é‡
                stats = {}
                for status in ProcessingStatus:
                    count = db.query(RawComment).filter(
                        RawComment.processing_status == status
                    ).count()
                    stats[status.value] = count
                
                # ç»Ÿè®¡å·²å¤„ç†è¯„è®ºçš„æ€»æ•°
                processed_total = db.query(ProcessedComment).count()
                stats["processed_results_total"] = processed_total
                
                self.logger.info(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: {stats}")
                return stats
                
        except Exception as e:
            self.logger.error(f"âŒ è·å–å¤„ç†ç»Ÿè®¡å¤±è´¥: {e}")
            raise


# åˆ›å»ºæœåŠ¡å®ä¾‹
comment_processing_service = CommentProcessingService()