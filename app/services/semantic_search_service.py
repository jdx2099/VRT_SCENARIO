"""
è¯­ä¹‰æœç´¢æœåŠ¡ - åŒæ­¥ç‰ˆæœ¬
ä¸“é—¨ç”¨äºCeleryä»»åŠ¡ï¼Œå®ç°è¯„è®ºæ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢
"""
import os
import json
import re
from typing import List, Dict, Optional, Tuple
from sqlalchemy.orm import Session
from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

from app.core.database import get_sync_session
from app.core.config import settings
from app.core.logging import app_logger
from app.models.comment_processing import ProductFeature
from app.models.raw_comment_update import RawComment, ProcessingStatus


class SemanticSearchService:
    """
    è¯­ä¹‰æœç´¢æœåŠ¡ç±» - åŒæ­¥ç‰ˆæœ¬
    ä¸“é—¨ç”¨äºCeleryä»»åŠ¡ï¼Œä½¿ç”¨pymysqlé©±åŠ¨
    """
    
    def __init__(self):
        self.logger = app_logger
        self.embeddings = None
        self.vector_store = None
        self.persist_directory = "/home/jdx/VRT_SCENARIO/db/vectorDB"
        self._initialize_embeddings()
    
    def _initialize_embeddings(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        try:
            self.embeddings = OpenAIEmbeddings(
                openai_api_base=settings.EMBEDDING_API_BASE,
                openai_api_key=settings.EMBEDDING_API_KEY,
                model=settings.EMBEDDING_MODEL_NAME
            )
            self.logger.info("âœ… åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"âŒ åˆå§‹åŒ–åµŒå…¥æ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def _load_product_features_from_db(self) -> List[Document]:
        """ä»æ•°æ®åº“åŠ è½½äº§å“åŠŸèƒ½å¹¶è½¬æ¢ä¸ºLangChain Documentå¯¹è±¡"""
        documents = []
        try:
            with get_sync_session() as db:
                features = db.query(ProductFeature).all()
                
                for feature in features:
                    if not feature.feature_name or not feature.feature_description:
                        self.logger.warning(f"è·³è¿‡ä¸å®Œæ•´çš„åŠŸèƒ½æ¨¡å—: ID={feature.product_feature_id}")
                        continue
                    
                    page_content = f"åŠŸèƒ½åç§°ï¼š{feature.feature_name}\nåŠŸèƒ½æè¿°ï¼š{feature.feature_description}"
                    metadata = {
                        "id": feature.feature_code,
                        "product_feature_id": feature.product_feature_id,
                        "åŠŸèƒ½æ¨¡å—åç§°": feature.feature_name,
                        "åŸå§‹æè¿°": feature.feature_description
                    }
                    doc = Document(page_content=page_content, metadata=metadata)
                    documents.append(doc)
                
                self.logger.info(f"ä»æ•°æ®åº“æˆåŠŸåŠ è½½äº† {len(documents)} ä¸ªåŠŸèƒ½æ¨¡å—")
                return documents
                
        except Exception as e:
            self.logger.error(f"âŒ ä»æ•°æ®åº“åŠ è½½äº§å“åŠŸèƒ½å¤±è´¥: {e}")
            raise
    
    def _create_vector_store(self) -> Chroma:
        """åˆ›å»ºæˆ–åŠ è½½å‘é‡å­˜å‚¨"""
        try:
            # ç¡®ä¿æŒä¹…åŒ–ç›®å½•å­˜åœ¨
            os.makedirs(self.persist_directory, exist_ok=True)
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æŒä¹…åŒ–çš„å‘é‡æ•°æ®åº“
            if os.path.exists(self.persist_directory) and os.listdir(self.persist_directory):
                self.logger.info(f"ğŸ”„ ä»æŒä¹…åŒ–å­˜å‚¨åŠ è½½å‘é‡æ•°æ®åº“: {self.persist_directory}")
                try:
                    vector_store = Chroma(
                        persist_directory=self.persist_directory,
                        embedding_function=self.embeddings
                    )
                    # éªŒè¯å‘é‡å­˜å‚¨æ˜¯å¦æœ‰æ•ˆ
                    collection = vector_store._collection
                    if collection.count() > 0:
                        self.logger.info(f"âœ… æˆåŠŸåŠ è½½å‘é‡æ•°æ®åº“ï¼ŒåŒ…å« {collection.count()} ä¸ªæ–‡æ¡£")
                        return vector_store
                    else:
                        self.logger.warning("âš ï¸ æŒä¹…åŒ–å‘é‡æ•°æ®åº“ä¸ºç©ºï¼Œå°†é‡æ–°åˆ›å»º")
                except Exception as load_error:
                    self.logger.warning(f"âš ï¸ åŠ è½½æŒä¹…åŒ–å‘é‡æ•°æ®åº“å¤±è´¥: {load_error}ï¼Œå°†é‡æ–°åˆ›å»º")
            
            # åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“
            documents = self._load_product_features_from_db()
            if not documents:
                raise ValueError("æ²¡æœ‰å¯ç”¨çš„äº§å“åŠŸèƒ½æ•°æ®")
            
            self.logger.info(f"ğŸ”¨ æ­£åœ¨åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨ï¼ŒåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£...")
            vector_store = Chroma.from_documents(
                documents=documents,
                embedding=self.embeddings,
                persist_directory=self.persist_directory
            )
            
            # æŒä¹…åŒ–å‘é‡æ•°æ®åº“
            vector_store.persist()
            self.logger.info(f"âœ… å‘é‡å­˜å‚¨åˆ›å»ºå¹¶æŒä¹…åŒ–æˆåŠŸï¼Œå­˜å‚¨è·¯å¾„: {self.persist_directory}")
            return vector_store
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ›å»º/åŠ è½½å‘é‡å­˜å‚¨å¤±è´¥: {e}")
            raise
    
    def get_vector_store(self) -> Chroma:
        """è·å–å‘é‡å­˜å‚¨å®ä¾‹"""
        if self.vector_store is None:
            self.vector_store = self._create_vector_store()
        return self.vector_store
    
    def split_comment_into_chunks(self, comment_text: str) -> List[Dict[str, str]]:
        """
        å°†è¯„è®ºæ–‡æœ¬æŒ‰ç« èŠ‚æ‹†åˆ†ä¸ºæ–‡æœ¬å—
        
        Args:
            comment_text: åŸå§‹è¯„è®ºæ–‡æœ¬
            
        Returns:
            æ–‡æœ¬å—åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«source_sectionå’Œchunk_text
        """
        chunks = []
        sections = re.split(r'(ã€[^ã€‘]+ã€‘)', comment_text)
        
        current_section_title = "è¯„è®ºå¼€å¤´"
        for part in sections:
            part = part.strip()
            if not part:
                continue
            
            if part.startswith("ã€") and part.endswith("ã€‘"):
                current_section_title = part
            else:
                chunk_text = part
                if len(chunk_text) > 5:  # è¿‡æ»¤å¤ªçŸ­çš„æ–‡æœ¬
                    chunks.append({
                        "source_section": current_section_title,
                        "chunk_text": chunk_text
                    })
        
        return chunks
    
    def search_similar_features(self, query_text: str, k: int = 1) -> List[Tuple[Document, float]]:
        """
        æœç´¢ä¸æŸ¥è¯¢æ–‡æœ¬æœ€ç›¸ä¼¼çš„åŠŸèƒ½æ¨¡å—
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›çš„ç»“æœæ•°é‡
            
        Returns:
            ç›¸ä¼¼åº¦æœç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(Document, score)
        """
        try:
            vector_store = self.get_vector_store()
            results = vector_store.similarity_search_with_score(query_text, k=k)
            return results
        except Exception as e:
            self.logger.error(f"âŒ è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            raise
    
    def process_comment_chunks(self, raw_comment_id: int, comment_text: str) -> List[Dict]:
        """
        å¤„ç†è¯„è®ºæ–‡æœ¬å—ï¼Œè¿›è¡Œè¯­ä¹‰æœç´¢
        
        Args:
            raw_comment_id: åŸå§‹è¯„è®ºID
            comment_text: è¯„è®ºæ–‡æœ¬
            
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        try:
            # æ‹†åˆ†æ–‡æœ¬å—
            chunks = self.split_comment_into_chunks(comment_text)
            self.logger.info(f"è¯„è®º {raw_comment_id} æ‹†åˆ†ä¸º {len(chunks)} ä¸ªæ–‡æœ¬å—")
            
            results = []
            
            for chunk in chunks:
                section_title = chunk["source_section"]
                chunk_text = chunk["chunk_text"]
                
                self.logger.debug(f"æ­£åœ¨å¤„ç†ç« èŠ‚: {section_title}")
                
                # è¿›è¡Œè¯­ä¹‰æœç´¢
                search_results = self.search_similar_features(chunk_text, k=1)
                
                if search_results:
                    doc, score = search_results[0]
                    
                    # æ£€æŸ¥ç›¸ä¼¼åº¦é˜ˆå€¼
                    if score < settings.SEMANTIC_SIMILARITY_THRESHOLD:
                        # ç”Ÿæˆè¯„è®ºç‰‡æ®µçš„å‘é‡
                        chunk_vector = self.embeddings.embed_query(chunk_text)
                        
                        result = {
                            "raw_comment_id": raw_comment_id,
                            "product_feature_id": doc.metadata.get("product_feature_id"),
                            "feature_similarity_score": float(score),
                            "comment_chunk_text": chunk_text,
                            "comment_chunk_vector": json.dumps(chunk_vector),
                            "feature_search_details": {
                                "source_section": section_title,
                                "matched_feature_code": doc.metadata.get("id"),
                                "matched_feature_name": doc.metadata.get("åŠŸèƒ½æ¨¡å—åç§°"),
                                "similarity_score": float(score),
                                "search_query_preview": chunk_text[:100] + "..." if len(chunk_text) > 100 else chunk_text
                            }
                        }
                        results.append(result)
                        
                        self.logger.info(f"âœ… æ‰¾åˆ°åŒ¹é…: {doc.metadata.get('åŠŸèƒ½æ¨¡å—åç§°')} (åˆ†æ•°: {score:.4f})")
                    else:
                        self.logger.debug(f"âŒ ç›¸ä¼¼åº¦è¿‡ä½: {score:.4f} >= {settings.SEMANTIC_SIMILARITY_THRESHOLD}")
                else:
                    self.logger.warning(f"æœªæ‰¾åˆ°åŒ¹é…çš„åŠŸèƒ½æ¨¡å—: {section_title}")
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ å¤„ç†è¯„è®ºæ–‡æœ¬å—å¤±è´¥: {e}")
            raise
    
    def get_pending_comments(self, limit: int = 20) -> List[RawComment]:
        """
        è·å–å¾…å¤„ç†çš„åŸå§‹è¯„è®º
        
        Args:
            limit: é™åˆ¶æ•°é‡
            
        Returns:
            å¾…å¤„ç†çš„åŸå§‹è¯„è®ºåˆ—è¡¨
        """
        try:
            with get_sync_session() as db:
                comments = db.query(RawComment).filter(
                    RawComment.processing_status == ProcessingStatus.NEW
                ).limit(limit).all()
                
                self.logger.info(f"è·å–åˆ° {len(comments)} æ¡å¾…å¤„ç†è¯„è®º")
                return comments
                
        except Exception as e:
            self.logger.error(f"âŒ è·å–å¾…å¤„ç†è¯„è®ºå¤±è´¥: {e}")
            raise
    
    def update_comment_status(self, raw_comment_id: int, status: ProcessingStatus):
        """
        æ›´æ–°è¯„è®ºå¤„ç†çŠ¶æ€
        
        Args:
            raw_comment_id: åŸå§‹è¯„è®ºID
            status: æ–°çŠ¶æ€
        """
        try:
            with get_sync_session() as db:
                comment = db.get(RawComment, raw_comment_id)
                if comment:
                    comment.processing_status = status
                    db.commit()
                    self.logger.debug(f"æ›´æ–°è¯„è®º {raw_comment_id} çŠ¶æ€ä¸º: {status.value}")
                else:
                    self.logger.warning(f"æœªæ‰¾åˆ°è¯„è®º: {raw_comment_id}")
                    
        except Exception as e:
            self.logger.error(f"âŒ æ›´æ–°è¯„è®ºçŠ¶æ€å¤±è´¥: {e}")
            raise


# åˆ›å»ºæœåŠ¡å®ä¾‹
semantic_search_service = SemanticSearchService()