"""
åŸå§‹è¯„è®ºæ›´æ–°æœåŠ¡ - åŒæ­¥ç‰ˆæœ¬
ä¸“é—¨ç”¨äºCeleryä»»åŠ¡ï¼Œé¿å…å¼‚æ­¥å†²çª
"""
from typing import List, Optional, Set
from sqlalchemy.orm import Session
from sqlalchemy import select, text, func
import httpx
import json
import time
import random
from datetime import datetime
from tqdm import tqdm

from app.core.database import get_sync_session
from app.core.logging import app_logger
from app.models.vehicle_update import VehicleChannelDetail, Channel
from app.models.raw_comment_update import RawComment, ProcessingStatus
from app.schemas.raw_comment_update import (
    RawCommentQueryRequest, RawCommentQueryResult, 
    VehicleChannelInfo, RawCommentCrawlRequest, RawCommentCrawlResult,
    NewCommentInfo
)


class RawCommentUpdateServiceSync:
    """
    åŸå§‹è¯„è®ºæ›´æ–°æœåŠ¡ç±» - åŒæ­¥ç‰ˆæœ¬
    ä¸“é—¨ç”¨äºCeleryä»»åŠ¡ï¼Œä½¿ç”¨pymysqlé©±åŠ¨
    """
    
    def __init__(self):
        self.logger = app_logger

    def get_vehicle_raw_comment_ids(self, query_request: RawCommentQueryRequest) -> RawCommentQueryResult:
        """
        æ ¹æ®æ¸ é“IDå’Œè½¦å‹æ ‡è¯†è·å–è¯¥è½¦å‹ä¸‹çš„æ‰€æœ‰åŸå§‹è¯„è®ºID - åŒæ­¥ç‰ˆæœ¬
        
        Args:
            query_request: æŸ¥è¯¢è¯·æ±‚å‚æ•°
            
        Returns:
            æŸ¥è¯¢ç»“æœï¼ŒåŒ…å«è½¦å‹ä¿¡æ¯å’Œè¯„è®ºIDåˆ—è¡¨
            
        Raises:
            ValueError: å½“è½¦å‹ä¸å­˜åœ¨æ—¶
        """
        try:
            with get_sync_session() as db:
                # ç¬¬ä¸€æ­¥ï¼šæ ¹æ®channel_idå’Œidentifier_on_channelæŸ¥è¯¢è½¦å‹æ¸ é“è¯¦æƒ…
                self.logger.info(f"ğŸ” æŸ¥è¯¢è½¦å‹: channel_id={query_request.channel_id}, identifier={query_request.identifier_on_channel}")
                
                vehicle_detail = db.query(VehicleChannelDetail).filter(
                    VehicleChannelDetail.channel_id_fk == query_request.channel_id,
                    VehicleChannelDetail.identifier_on_channel == query_request.identifier_on_channel
                ).first()
                
                if not vehicle_detail:
                    raise ValueError(f"æœªæ‰¾åˆ°åŒ¹é…çš„è½¦å‹: channel_id={query_request.channel_id}, identifier={query_request.identifier_on_channel}")
                
                self.logger.info(f"âœ… æ‰¾åˆ°è½¦å‹: vehicle_channel_id={vehicle_detail.vehicle_channel_id}, name={vehicle_detail.name_on_channel}")
                
                # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨vehicle_channel_idæŸ¥è¯¢æ‰€æœ‰ç›¸å…³çš„åŸå§‹è¯„è®ºID
                raw_comment_ids = db.query(RawComment.raw_comment_id).filter(
                    RawComment.vehicle_channel_id_fk == vehicle_detail.vehicle_channel_id
                ).order_by(RawComment.raw_comment_id).all()
                
                raw_comment_ids = [row[0] for row in raw_comment_ids]  # æå–IDå€¼
                
                self.logger.info(f"ğŸ“Š æ‰¾åˆ° {len(raw_comment_ids)} æ¡åŸå§‹è¯„è®º")
                
                # æ„å»ºè½¦å‹æ¸ é“ä¿¡æ¯
                vehicle_channel_info = VehicleChannelInfo(
                    vehicle_channel_id=vehicle_detail.vehicle_channel_id,
                    channel_id=vehicle_detail.channel_id_fk,
                    identifier_on_channel=vehicle_detail.identifier_on_channel,
                    name_on_channel=vehicle_detail.name_on_channel,
                    url_on_channel=vehicle_detail.url_on_channel,
                    temp_brand_name=vehicle_detail.temp_brand_name,
                    temp_series_name=vehicle_detail.temp_series_name,
                    temp_model_year=vehicle_detail.temp_model_year,
                    last_comment_crawled_at=vehicle_detail.last_comment_crawled_at
                )
                
                # æ„å»ºæŸ¥è¯¢ç»“æœ
                result = RawCommentQueryResult(
                    vehicle_channel_info=vehicle_channel_info,
                    raw_comment_ids=raw_comment_ids,
                    total_comments=len(raw_comment_ids)
                )
                
                return result
                
        except Exception as e:
            self.logger.error(f"âŒ æŸ¥è¯¢è½¦å‹åŸå§‹è¯„è®ºIDå¤±è´¥: {e}")
            raise
    
    def get_vehicle_by_channel_and_identifier(self, channel_id: int, identifier_on_channel: str) -> Optional[VehicleChannelDetail]:
        """
        æ ¹æ®æ¸ é“IDå’Œè½¦å‹æ ‡è¯†è·å–è½¦å‹è¯¦æƒ… - åŒæ­¥ç‰ˆæœ¬
        
        Args:
            channel_id: æ¸ é“ID
            identifier_on_channel: è½¦å‹åœ¨æ¸ é“ä¸Šçš„æ ‡è¯†
            
        Returns:
            è½¦å‹æ¸ é“è¯¦æƒ…å¯¹è±¡ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        try:
            with get_sync_session() as db:
                vehicle_detail = db.query(VehicleChannelDetail).filter(
                    VehicleChannelDetail.channel_id_fk == channel_id,
                    VehicleChannelDetail.identifier_on_channel == identifier_on_channel
                ).first()
                return vehicle_detail
        except Exception as e:
            self.logger.error(f"âŒ æŸ¥è¯¢è½¦å‹è¯¦æƒ…å¤±è´¥: {e}")
            raise
    
    def count_raw_comments_by_vehicle_channel_id(self, vehicle_channel_id: int) -> int:
        """
        ç»Ÿè®¡æŒ‡å®šè½¦å‹æ¸ é“è¯¦æƒ…IDä¸‹çš„åŸå§‹è¯„è®ºæ•°é‡ - åŒæ­¥ç‰ˆæœ¬
        
        Args:
            vehicle_channel_id: è½¦å‹æ¸ é“è¯¦æƒ…ID
            
        Returns:
            è¯„è®ºæ•°é‡
        """
        try:
            with get_sync_session() as db:
                count = db.query(func.count(RawComment.raw_comment_id)).filter(
                    RawComment.vehicle_channel_id_fk == vehicle_channel_id
                ).scalar()
                return count or 0
        except Exception as e:
            self.logger.error(f"âŒ ç»Ÿè®¡åŸå§‹è¯„è®ºæ•°é‡å¤±è´¥: {e}")
            raise
    
    def get_vehicles_by_channel(self, channel_id: int) -> List[VehicleChannelDetail]:
        """
        è·å–æŒ‡å®šæ¸ é“ä¸‹çš„æ‰€æœ‰è½¦å‹ - åŒæ­¥ç‰ˆæœ¬
        
        Args:
            channel_id: æ¸ é“ID
            
        Returns:
            è½¦å‹åˆ—è¡¨
        """
        try:
            with get_sync_session() as db:
                vehicles = db.query(VehicleChannelDetail).filter(
                    VehicleChannelDetail.channel_id_fk == channel_id
                ).order_by(VehicleChannelDetail.name_on_channel).all()
                
                self.logger.info(f"ğŸ“Š è·å–åˆ°æ¸ é“ {channel_id} ä¸‹çš„ {len(vehicles)} ä¸ªè½¦å‹")
                return vehicles
                
        except Exception as e:
            self.logger.error(f"âŒ è·å–æ¸ é“è½¦å‹åˆ—è¡¨å¤±è´¥: {e}")
            raise
    
    def crawl_new_comments(self, crawl_request: RawCommentCrawlRequest) -> RawCommentCrawlResult:
        """
        çˆ¬å–æ–°çš„åŸå§‹è¯„è®º - åŒæ­¥ç‰ˆæœ¬
        
        Args:
            crawl_request: çˆ¬å–è¯·æ±‚å‚æ•°
            
        Returns:
            çˆ¬å–ç»“æœï¼ŒåŒ…å«æ–°å¢è¯„è®ºä¿¡æ¯
        """
        start_time = time.time()
        
        try:
            with get_sync_session() as db:
                # ç¬¬ä¸€æ­¥ï¼šè·å–è½¦å‹ä¿¡æ¯
                self.logger.info(f"ğŸ” å¼€å§‹çˆ¬å–è¯„è®º: channel_id={crawl_request.channel_id}, identifier={crawl_request.identifier_on_channel}")
                
                vehicle_detail = self._get_vehicle_detail(db, crawl_request.channel_id, crawl_request.identifier_on_channel)
                if not vehicle_detail:
                    raise ValueError(f"æœªæ‰¾åˆ°åŒ¹é…çš„è½¦å‹: channel_id={crawl_request.channel_id}, identifier={crawl_request.identifier_on_channel}")
                
                # ç¬¬äºŒæ­¥ï¼šè·å–æ¸ é“é…ç½®
                channel_config = self._get_channel_config(db, crawl_request.channel_id)
                if not channel_config:
                    raise ValueError(f"æœªæ‰¾åˆ°æ¸ é“é…ç½®: channel_id={crawl_request.channel_id}")
                
                # ç¬¬ä¸‰æ­¥ï¼šè·å–å·²æœ‰è¯„è®ºIDåˆ—è¡¨
                existing_comment_ids = self._get_existing_comment_identifiers(db, vehicle_detail.vehicle_channel_id)
                self.logger.info(f"ğŸ“Š æ•°æ®åº“ä¸­å·²æœ‰ {len(existing_comment_ids)} æ¡è¯„è®º")
                
                # ç¬¬å››æ­¥ï¼šè·å–è¯„è®ºæ€»é¡µæ•°
                total_pages = self._count_pages(channel_config, crawl_request.identifier_on_channel)
                self.logger.info(f"ğŸ“„ å…±å‘ç° {total_pages} é¡µè¯„è®º")
                
                # é™åˆ¶æœ€å¤§çˆ¬å–é¡µæ•°
                max_pages = min(total_pages, crawl_request.max_pages or total_pages)
                
                # ç¬¬äº”æ­¥ï¼šçˆ¬å–æ–°è¯„è®º
                new_comments = self._collect_new_comments(
                    channel_config, 
                    crawl_request.identifier_on_channel,
                    max_pages,
                    existing_comment_ids,
                    vehicle_detail.vehicle_channel_id
                )
                
                # ç¬¬å…­æ­¥ï¼šçˆ¬å–è¯„è®ºè¯¦ç»†å†…å®¹
                if new_comments:
                    self.logger.info(f"ğŸ“ å¼€å§‹çˆ¬å– {len(new_comments)} æ¡è¯„è®ºçš„è¯¦ç»†å†…å®¹...")
                    self._scrape_comments_contents(new_comments, channel_config)
                
                # ç¬¬ä¸ƒæ­¥ï¼šä¿å­˜æ–°è¯„è®ºåˆ°æ•°æ®åº“
                saved_count = self._save_new_comments(db, new_comments, vehicle_detail.vehicle_channel_id)
                
                # æ„å»ºè½¦å‹æ¸ é“ä¿¡æ¯
                vehicle_channel_info = VehicleChannelInfo(
                    vehicle_channel_id=vehicle_detail.vehicle_channel_id,
                    channel_id=vehicle_detail.channel_id_fk,
                    identifier_on_channel=vehicle_detail.identifier_on_channel,
                    name_on_channel=vehicle_detail.name_on_channel,
                    url_on_channel=vehicle_detail.url_on_channel,
                    temp_brand_name=vehicle_detail.temp_brand_name,
                    temp_series_name=vehicle_detail.temp_series_name,
                    temp_model_year=vehicle_detail.temp_model_year,
                    last_comment_crawled_at=vehicle_detail.last_comment_crawled_at
                )
                
                crawl_duration = time.time() - start_time
                
                result = RawCommentCrawlResult(
                    vehicle_channel_info=vehicle_channel_info,
                    total_pages_crawled=max_pages,
                    total_comments_found=len(new_comments),
                    new_comments_count=saved_count,
                    new_comments=[
                        NewCommentInfo(
                            identifier_on_channel=comment["identifier_on_channel"],
                            comment_content=comment["comment_content"],
                            posted_at_on_channel=comment["posted_at_on_channel"],
                            comment_source_url=comment.get("comment_source_url")
                        ) for comment in new_comments[:10]  # åªè¿”å›å‰10æ¡ç”¨äºå±•ç¤º
                    ],
                    crawl_duration=round(crawl_duration, 2)
                )
                
                self.logger.info(f"âœ… çˆ¬å–å®Œæˆ: å‘ç° {len(new_comments)} æ¡æ–°è¯„è®º, ä¿å­˜ {saved_count} æ¡, è€—æ—¶ {crawl_duration:.2f}ç§’")
                
                return result
                
        except Exception as e:
            self.logger.error(f"âŒ çˆ¬å–è¯„è®ºå¤±è´¥: {e}")
            raise
    
    def _get_vehicle_detail(self, db: Session, channel_id: int, identifier_on_channel: str) -> Optional[VehicleChannelDetail]:
        """è·å–è½¦å‹è¯¦æƒ… - åŒæ­¥ç‰ˆæœ¬"""
        return db.query(VehicleChannelDetail).filter(
            VehicleChannelDetail.channel_id_fk == channel_id,
            VehicleChannelDetail.identifier_on_channel == identifier_on_channel
        ).first()
    
    def _get_channel_config(self, db: Session, channel_id: int) -> Optional[dict]:
        """è·å–æ¸ é“é…ç½® - åŒæ­¥ç‰ˆæœ¬"""
        channel = db.query(Channel).filter(Channel.channel_id == channel_id).first()
        if not channel or not channel.channel_base_url:
            return None
        
        try:
            # è§£æchannel_base_urlä¸­çš„JSONé…ç½®
            config = json.loads(channel.channel_base_url)
            return config
        except json.JSONDecodeError:
            self.logger.error(f"âŒ æ¸ é“é…ç½®JSONè§£æå¤±è´¥: channel_id={channel_id}, content={channel.channel_base_url}")
            return None
    
    def _get_existing_comment_identifiers(self, db: Session, vehicle_channel_id: int) -> Set[str]:
        """è·å–å·²æœ‰è¯„è®ºæ ‡è¯†ç¬¦é›†åˆ - åŒæ­¥ç‰ˆæœ¬"""
        identifiers = db.query(RawComment.identifier_on_channel).filter(
            RawComment.vehicle_channel_id_fk == vehicle_channel_id
        ).all()
        return set([row[0] for row in identifiers])
    
    def _count_pages(self, channel_config: dict, identifier: str) -> int:
        """è·å–è¯„è®ºæ€»é¡µæ•° - åŒæ­¥ç‰ˆæœ¬"""
        try:
            koubei_config = channel_config.get("koubei_series", {})
            url_template = koubei_config.get("url", "")
            
            if not url_template:
                raise ValueError("æ¸ é“é…ç½®ä¸­æœªæ‰¾åˆ°koubei_series.url")
            
            # æ¸…ç†URLæ¨¡æ¿å¹¶æ ¼å¼åŒ–ç¬¬ä¸€é¡µURL
            clean_template = url_template.strip()
            try:
                first_page_url = clean_template.format(identifier, 1)
            except (IndexError, ValueError) as e:
                self.logger.error(f"âŒ URLæ¨¡æ¿æ ¼å¼åŒ–å¤±è´¥: {e}")
                return 1
            
            with httpx.Client(timeout=30.0) as client:
                response = client.get(first_page_url)
                response.raise_for_status()
                
                data = response.json()
                # å°è¯•å¤šç§å¯èƒ½çš„é¡µæ•°å­—æ®µå
                result = data.get("result", {})
                total_pages = (result.get("pagecount", 0) or 
                             result.get("totalpage", 0) or 
                             result.get("total_page", 0) or 1)
                
                self.logger.info(f"ğŸ“„ APIè¿”å›æ€»é¡µæ•°: {total_pages}")
                return total_pages
                
        except (IndexError, ValueError, KeyError) as format_error:
            self.logger.error(f"âŒ URLæ ¼å¼åŒ–é”™è¯¯: {format_error}, template='{url_template}', identifier='{identifier}'")
            return 1
        except Exception as e:
            self.logger.error(f"âŒ è·å–è¯„è®ºæ€»é¡µæ•°å¤±è´¥: {e}")
            return 1  # é»˜è®¤è¿”å›1é¡µ
    
    def _collect_new_comments(
        self, 
        channel_config: dict, 
        identifier: str, 
        max_pages: int,
        existing_identifiers: Set[str],
        vehicle_channel_id: int
    ) -> List[dict]:
        """æ”¶é›†æ–°è¯„è®º - åŒæ­¥ç‰ˆæœ¬"""
        koubei_config = channel_config.get("koubei_series", {})
        url_template = koubei_config.get("url", "")
        
        if not url_template:
            raise ValueError("æ¸ é“é…ç½®ä¸­æœªæ‰¾åˆ°koubei_series.url")
        
        new_comments = []
        seen_identifiers = set()
        
        self.logger.info(f"ğŸ•·ï¸ å¼€å§‹çˆ¬å– {max_pages} é¡µè¯„è®º...")
        
        # æ¸…ç†URLæ¨¡æ¿
        clean_template = url_template.strip()
        
        with httpx.Client(timeout=30.0) as client:
            for page in tqdm(range(1, max_pages + 1), desc="çˆ¬å–è¯„è®ºé¡µé¢"):
                try:
                    # æ„å»ºé¡µé¢URL
                    try:
                        page_url = clean_template.format(identifier, page)
                    except (IndexError, ValueError) as e:
                        self.logger.error(f"âŒ URLæ ¼å¼åŒ–é”™è¯¯: {e}")
                        continue
                    
                    response = client.get(page_url)
                    response.raise_for_status()
                    
                    data = response.json()
                    comments = data.get("result", {}).get("list", [])
                    
                    if not comments:
                        self.logger.info(f"ğŸ“„ ç¬¬ {page} é¡µæ— è¯„è®ºæ•°æ®ï¼Œåœæ­¢çˆ¬å–")
                        break
                    
                    page_new_count = 0
                    
                    for i, comment in enumerate(comments):
                        # å°è¯•å¤šç§å¯èƒ½çš„IDå­—æ®µå (æ³¨æ„å¤§å°å†™)
                        comment_id = str(comment.get("id", "") or 
                                      comment.get("Koubeiid", "") or  # æ­£ç¡®çš„å­—æ®µå
                                      comment.get("koubeiId", "") or 
                                      comment.get("alibiId", "") or 
                                      comment.get("commentId", "") or 
                                      comment.get("uuid", "") or "")
                        
                        if not comment_id:
                            continue
                        
                        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æˆ–å·²å¤„ç†
                        if comment_id in existing_identifiers or comment_id in seen_identifiers:
                            continue
                        
                        # è§£æè¯„è®ºåŸºæœ¬æ•°æ®ï¼ˆå†…å®¹å°†åœ¨åç»­æ­¥éª¤ä¸­çˆ¬å–ï¼‰
                        comment_data = {
                            "identifier_on_channel": comment_id,
                            "comment_content": "",  # å†…å®¹å°†åœ¨è¯¦æƒ…çˆ¬å–æ­¥éª¤ä¸­å¡«å……
                            "posted_at_on_channel": self._parse_post_time(comment.get("posttime", "")),
                            "comment_source_url": ""  # URLå°†åœ¨è¯¦æƒ…çˆ¬å–æ­¥éª¤ä¸­è®¾ç½®
                        }
                        

                        
                        new_comments.append(comment_data)
                        seen_identifiers.add(comment_id)
                        page_new_count += 1
                    
                    self.logger.info(f"ğŸ“„ ç¬¬ {page} é¡µ: å‘ç° {len(comments)} æ¡è¯„è®º, æ–°å¢ {page_new_count} æ¡")
                    
                    # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                    time.sleep(random.uniform(0.5, 1.5))
                    
                except Exception as e:
                    self.logger.error(f"âŒ çˆ¬å–ç¬¬ {page} é¡µå¤±è´¥: {e}")
                    continue
        
        self.logger.info(f"ğŸ‰ è¯„è®ºæ”¶é›†å®Œæˆ: æ€»å…±å‘ç° {len(new_comments)} æ¡æ–°è¯„è®º")
        return new_comments
    
    def _parse_post_time(self, post_time_str: str) -> Optional[datetime]:
        """è§£æå‘å¸ƒæ—¶é—´å­—ç¬¦ä¸²"""
        if not post_time_str or not post_time_str.strip():
            return None
        
        try:
            # å¦‚æœæ—¶é—´æ ¼å¼æ˜¯ '2025-07-22'ï¼Œéœ€è¦è½¬æ¢ä¸ºå®Œæ•´çš„datetime
            if len(post_time_str) == 10:  # YYYY-MM-DDæ ¼å¼
                return datetime.strptime(post_time_str, "%Y-%m-%d")
            else:
                return datetime.strptime(post_time_str, "%Y-%m-%d %H:%M:%S")
        except:
            return None

    def _scrape_comments_contents(self, new_comments: List[dict], channel_config: dict):
        """
        çˆ¬å–è¯„è®ºè¯¦ç»†å†…å®¹ - åŒæ­¥ç‰ˆæœ¬
        
        å‚æ•°ï¼š
            new_comments: æ–°è¯„è®ºåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« identifier_on_channel ç­‰å­—æ®µ
            channel_config: æ¸ é“é…ç½®ï¼ŒåŒ…å« koubei_detail.url æ¨¡æ¿
        """
        try:
            # è·å–è¯¦æƒ…APIé…ç½®
            koubei_detail_config = channel_config.get("koubei_detail", {})
            detail_url_template = koubei_detail_config.get("url", "")
            
            if not detail_url_template:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ° koubei_detail.url é…ç½®ï¼Œè·³è¿‡å†…å®¹çˆ¬å–")
                return
            
            self.logger.info(f"ğŸ”§ ä½¿ç”¨è¯¦æƒ…APIæ¨¡æ¿: {detail_url_template}")
            
            with httpx.Client(timeout=30.0) as client:
                for i, comment_data in enumerate(new_comments):
                    koubei_id = comment_data["identifier_on_channel"]
                    
                    try:
                        # çˆ¬å–å•ä¸ªè¯„è®ºè¯¦ç»†å†…å®¹
                        content = self._scrape_single_comment_content(
                            client, koubei_id, detail_url_template
                        )
                        
                        # æ›´æ–°è¯„è®ºæ•°æ®
                        comment_data["comment_content"] = content
                        comment_data["comment_source_url"] = detail_url_template.format(koubei_id)
                        
                        self.logger.info(f"ğŸ“ [{i+1}/{len(new_comments)}] æˆåŠŸçˆ¬å–è¯„è®ºå†…å®¹ - KoubeiID: {koubei_id}")
                        
                        # æ·»åŠ å»¶è¿Ÿé¿å…åçˆ¬è™«
                        if i < len(new_comments) - 1:
                            time.sleep(random.uniform(1.0, 1.5))
                            
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ [{i+1}/{len(new_comments)}] çˆ¬å–å¤±è´¥ - KoubeiID: {koubei_id}, é”™è¯¯: {e}")
                        # è®¾ç½®é»˜è®¤å€¼ï¼Œé¿å…ä¿å­˜æ—¶å‡ºé”™
                        comment_data["comment_content"] = ""
                        comment_data["comment_source_url"] = detail_url_template.format(koubei_id)
            
            self.logger.info(f"âœ… è¯„è®ºå†…å®¹çˆ¬å–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ è¯„è®ºå†…å®¹çˆ¬å–å¤±è´¥: {e}")
            # ä¸ºæ‰€æœ‰è¯„è®ºè®¾ç½®é»˜è®¤å€¼
            for comment_data in new_comments:
                if "comment_content" not in comment_data:
                    comment_data["comment_content"] = ""

    def _scrape_single_comment_content(
        self, 
        client: httpx.Client, 
        koubei_id: str, 
        url_template: str
    ) -> str:
        """
        çˆ¬å–å•ä¸ªè¯„è®ºçš„è¯¦ç»†å†…å®¹ - åŒæ­¥ç‰ˆæœ¬
        
        å‚æ•°ï¼š
            client: HTTPå®¢æˆ·ç«¯
            koubei_id: å£ç¢‘ID
            url_template: URLæ¨¡æ¿
            
        è¿”å›ï¼š
            è¯„è®ºå†…å®¹å­—ç¬¦ä¸²
        """
        try:
            # æ„å»ºè¯¦æƒ…URL
            detail_url = url_template.format(koubei_id)
            
            # å‘é€è¯·æ±‚
            response = client.get(detail_url)
            response.raise_for_status()
            
            # è§£æJSONæ•°æ®
            data = response.json()
            
            # æå–è¯„è®ºå†…å®¹
            if data and "result" in data and "content" in data["result"]:
                content = data["result"]["content"]
                if content and content.strip():
                    return content.strip()
                else:
                    self.logger.debug(f"ğŸ“„ KoubeiID {koubei_id} å†…å®¹ä¸ºç©º")
                    return ""
            else:
                self.logger.warning(f"âš ï¸ KoubeiID {koubei_id} JSONæ ¼å¼å¼‚å¸¸: {data}")
                return ""
                
        except httpx.HTTPStatusError as e:
            self.logger.warning(f"âš ï¸ HTTPé”™è¯¯ - KoubeiID: {koubei_id}, çŠ¶æ€ç : {e.response.status_code}")
            return ""
        except Exception as e:
            self.logger.warning(f"âš ï¸ è¯·æ±‚å¼‚å¸¸ - KoubeiID: {koubei_id}, é”™è¯¯: {e}")
            return ""
    
    def _save_new_comments(self, db: Session, new_comments: List[dict], vehicle_channel_id: int) -> int:
        """ä¿å­˜æ–°è¯„è®ºåˆ°æ•°æ®åº“ - åŒæ­¥ç‰ˆæœ¬"""
        if not new_comments:
            return 0
        
        try:
            saved_count = 0
            
            for comment_data in new_comments:
                comment = RawComment(
                    vehicle_channel_id_fk=vehicle_channel_id,
                    identifier_on_channel=comment_data["identifier_on_channel"],
                    comment_content=comment_data["comment_content"],
                    posted_at_on_channel=comment_data["posted_at_on_channel"],
                    comment_source_url=comment_data["comment_source_url"],
                    # æ–°å¢ï¼šè®¾ç½®å¤„ç†çŠ¶æ€ä¸ºæ–°å»ºçŠ¶æ€
                    processing_status=ProcessingStatus.NEW
                )
                
                db.add(comment)
                saved_count += 1
            
            db.commit()
            self.logger.info(f"ğŸ’¾ æˆåŠŸä¿å­˜ {saved_count} æ¡æ–°è¯„è®ºåˆ°æ•°æ®åº“")
            
            return saved_count
            
        except Exception as e:
            db.rollback()
            self.logger.error(f"âŒ ä¿å­˜è¯„è®ºå¤±è´¥: {e}")
            raise


# å…¨å±€æœåŠ¡å®ä¾‹
raw_comment_update_service_sync = RawCommentUpdateServiceSync() 