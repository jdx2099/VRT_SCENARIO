"""
å®šæ—¶è¯„è®ºè¯­ä¹‰å¤„ç†ä»»åŠ¡æ¨¡å— - åŒæ­¥ç‰ˆæœ¬
åŸºäºCelery Beatå®ç°å‘¨æœŸæ€§è¯„è®ºè¯­ä¹‰åˆ†æå’Œç»“æ„åŒ–æå–ä»»åŠ¡
"""
from celery import current_task
from app.tasks.celery_app import celery_app
from app.core.logging import app_logger
from datetime import datetime, timezone
from typing import Dict, Optional
import time


@celery_app.task(bind=True, max_retries=3)
def scheduled_comment_semantic_processing(self, batch_size: int = 20):
    """
    å®šæ—¶è¯„è®ºè¯­ä¹‰å¤„ç†ä»»åŠ¡ - åŒæ­¥ç‰ˆæœ¬
    
    ä»raw_commentsè¡¨ä¸­æ‰¾åˆ°processing_statusä¸º'new'çš„è¯„è®ºï¼Œ
    æ¯æ¬¡å¤„ç†æŒ‡å®šæ•°é‡çš„è¯„è®ºï¼Œè¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢å’Œç»“æ„åŒ–æå–
    
    Args:
        batch_size: æ¯æ‰¹å¤„ç†çš„è¯„è®ºæ•°é‡ï¼Œé»˜è®¤20æ¡
    """
    from app.core.database import get_sync_session
    from app.models.vehicle_update import ProcessingJob
    from app.services.comment_processing_service import comment_processing_service
    
    try:
        app_logger.info(f"â° å¼€å§‹æ‰§è¡Œå®šæ—¶è¯„è®ºè¯­ä¹‰å¤„ç†ä»»åŠ¡: batch_size={batch_size}")
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰å¯¹åº”çš„ProcessingJobè®°å½•ï¼ˆé¿å…é‡å¤åˆ›å»ºï¼‰
        job_id = None
        celery_task_id = self.request.id
        
        try:
            with get_sync_session() as db:
                # æŸ¥æ‰¾æ˜¯å¦å·²æœ‰ç›¸åŒcelery_task_idçš„è®°å½•
                existing_job = db.query(ProcessingJob).filter(
                    ProcessingJob.job_type == "scheduled_comment_semantic_processing",
                    ProcessingJob.parameters.contains({"celery_task_id": celery_task_id})
                ).first()
                
                if existing_job:
                    # å¦‚æœæ‰¾åˆ°ç°æœ‰è®°å½•ï¼Œä½¿ç”¨å®ƒ
                    job_id = existing_job.job_id
                    app_logger.info(f"ğŸ”„ å‘ç°ç°æœ‰ä»»åŠ¡è®°å½•ï¼Œç»§ç»­æ‰§è¡Œ: job_id={job_id}, celery_task_id={celery_task_id}")
                    
                    # å¦‚æœçŠ¶æ€æ˜¯runningï¼Œè¯´æ˜ä»»åŠ¡è¢«ä¸­æ–­åé‡æ–°å¯åŠ¨
                    if existing_job.status == "running":
                        app_logger.info(f"ğŸ”„ ä»»åŠ¡è¢«ä¸­æ–­åé‡æ–°å¯åŠ¨ï¼Œç»§ç»­æ‰§è¡Œ: job_id={job_id}")
                else:
                    # åˆ›å»ºæ–°çš„ä»»åŠ¡è®°å½•
                    processing_job = ProcessingJob(
                        job_type="scheduled_comment_semantic_processing",
                        status="running",
                        parameters={
                            "batch_size": batch_size,
                            "celery_task_id": celery_task_id
                        },
                        pipeline_version="1.0.0",
                        created_by_user_id_fk=None,
                        started_at=datetime.now(timezone.utc)
                    )
                    db.add(processing_job)
                    db.commit()
                    db.refresh(processing_job)
                    job_id = processing_job.job_id
                    app_logger.info(f"ğŸ“ åˆ›å»ºæ–°çš„è¯„è®ºè¯­ä¹‰å¤„ç†ä»»åŠ¡è®°å½•: job_id={job_id}")
            
        except Exception as e:
            app_logger.error(f"âŒ å¤„ç†ä»»åŠ¡è®°å½•å¤±è´¥: {e}")
            raise
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        current_task.update_state(
            state='PROGRESS',
            meta={
                'current': 0,
                'total': batch_size,
                'progress': 0,
                'status': 'æ­£åœ¨æŸ¥è¯¢å¾…å¤„ç†è¯„è®º...',
                'batch_size': batch_size,
                'job_id': job_id,
                'celery_task_id': celery_task_id
            }
        )
        
        # è·å–å¤„ç†å‰çš„ç»Ÿè®¡ä¿¡æ¯
        try:
            pre_stats = comment_processing_service.get_processing_statistics()
            app_logger.info(f"ğŸ“Š å¤„ç†å‰ç»Ÿè®¡: {pre_stats}")
        except Exception as e:
            app_logger.warning(f"âš ï¸ è·å–å¤„ç†å‰ç»Ÿè®¡å¤±è´¥: {e}")
            pre_stats = {}
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        current_task.update_state(
            state='PROGRESS',
            meta={
                'current': 0,
                'total': batch_size,
                'progress': 0,
                'status': f'å¼€å§‹å¤„ç† {batch_size} æ¡è¯„è®ºçš„è¯­ä¹‰åˆ†æ',
                'pre_stats': pre_stats
            }
        )
        
        # æ‰§è¡Œæ‰¹é‡å¤„ç†
        start_time = time.time()
        
        try:
            processing_result = comment_processing_service.process_batch_comments(
                limit=batch_size,
                job_id=job_id
            )
            
            processing_duration = time.time() - start_time
            
            # è·å–å¤„ç†åçš„ç»Ÿè®¡ä¿¡æ¯
            try:
                post_stats = comment_processing_service.get_processing_statistics()
                app_logger.info(f"ğŸ“Š å¤„ç†åç»Ÿè®¡: {post_stats}")
            except Exception as e:
                app_logger.warning(f"âš ï¸ è·å–å¤„ç†åç»Ÿè®¡å¤±è´¥: {e}")
                post_stats = {}
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
            try:
                with get_sync_session() as db:
                    job = db.get(ProcessingJob, job_id)
                    if job:
                        job.status = "completed"
                        job.completed_at = datetime.now(timezone.utc)
                        job.result_summary = f"è¯„è®ºè¯­ä¹‰å¤„ç†å®Œæˆ: å¤„ç†{processing_result['processed_count']}æ¡ï¼Œè·³è¿‡{processing_result['skipped_count']}æ¡ï¼Œå¤±è´¥{processing_result['failed_count']}æ¡ï¼Œç”Ÿæˆ{processing_result['total_results']}æ¡ç»“æœ"
                        db.commit()
            except Exception as e:
                app_logger.error(f"âŒ æ›´æ–°ä»»åŠ¡è®°å½•å¤±è´¥: {e}")
            
            # æ„å»ºæœ€ç»ˆç»“æœ
            final_result = {
                'status': 'completed',
                'job_id': job_id,
                'celery_task_id': celery_task_id,
                'processing_duration': processing_duration,
                'batch_size': batch_size,
                'processing_result': processing_result,
                'pre_stats': pre_stats,
                'post_stats': post_stats,
                'message': f"è¯„è®ºè¯­ä¹‰å¤„ç†ä»»åŠ¡å®Œæˆ: å¤„ç†äº†{processing_result['total_comments']}æ¡è¯„è®ºï¼Œç”Ÿæˆ{processing_result['total_results']}æ¡ç»“æ„åŒ–ç»“æœ"
            }
            
            app_logger.info(f"âœ… å®šæ—¶è¯„è®ºè¯­ä¹‰å¤„ç†ä»»åŠ¡å®Œæˆ: {final_result}")
            
            # æ›´æ–°æœ€ç»ˆä»»åŠ¡çŠ¶æ€
            current_task.update_state(
                state='SUCCESS',
                meta=final_result
            )
            
            return final_result
            
        except Exception as e:
            app_logger.error(f"âŒ æ‰¹é‡å¤„ç†è¯„è®ºå¤±è´¥: {e}")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
            try:
                with get_sync_session() as db:
                    job = db.get(ProcessingJob, job_id)
                    if job:
                        job.status = "failed"
                        job.completed_at = datetime.now(timezone.utc)
                        job.result_summary = f"è¯„è®ºè¯­ä¹‰å¤„ç†å¤±è´¥: {str(e)}"
                        db.commit()
            except Exception as update_e:
                app_logger.error(f"âŒ æ›´æ–°å¤±è´¥ä»»åŠ¡è®°å½•å¤±è´¥: {update_e}")
            
            raise
        
    except Exception as e:
        app_logger.error(f"âŒ å®šæ—¶è¯„è®ºè¯­ä¹‰å¤„ç†ä»»åŠ¡å¤±è´¥: {e}")
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        current_task.update_state(
            state='FAILURE',
            meta={
                'status': 'failed',
                'error': str(e),
                'message': f'å®šæ—¶è¯„è®ºè¯­ä¹‰å¤„ç†ä»»åŠ¡å¤±è´¥: {e}'
            }
        )
        
        raise


@celery_app.task(bind=True)
def get_comment_processing_status(self, job_id: Optional[int] = None):
    """
    è·å–è¯„è®ºå¤„ç†çŠ¶æ€ç»Ÿè®¡
    
    Args:
        job_id: å¯é€‰çš„ä»»åŠ¡IDï¼Œç”¨äºè·å–ç‰¹å®šä»»åŠ¡çš„è¯¦æƒ…
    """
    from app.core.database import get_sync_session
    from app.models.vehicle_update import ProcessingJob
    from app.services.comment_processing_service import comment_processing_service
    
    try:
        app_logger.info(f"ğŸ“Š è·å–è¯„è®ºå¤„ç†çŠ¶æ€ç»Ÿè®¡: job_id={job_id}")
        
        # è·å–åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        stats = comment_processing_service.get_processing_statistics()
        
        result = {
            'status': 'success',
            'statistics': stats,
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
        
        # å¦‚æœæŒ‡å®šäº†job_idï¼Œè·å–ä»»åŠ¡è¯¦æƒ…
        if job_id:
            try:
                with get_sync_session() as db:
                    job = db.get(ProcessingJob, job_id)
                    if job:
                        result['job_details'] = {
                            'job_id': job.job_id,
                            'job_type': job.job_type,
                            'status': job.status,
                            'parameters': job.parameters,
                            'created_at': job.created_at.isoformat() if job.created_at else None,
                            'started_at': job.started_at.isoformat() if job.started_at else None,
                            'completed_at': job.completed_at.isoformat() if job.completed_at else None,
                            'result_summary': job.result_summary,
                            'pipeline_version': job.pipeline_version
                        }
                    else:
                        result['job_details'] = None
                        result['message'] = f"æœªæ‰¾åˆ°ä»»åŠ¡ID: {job_id}"
            except Exception as e:
                app_logger.error(f"âŒ è·å–ä»»åŠ¡è¯¦æƒ…å¤±è´¥: {e}")
                result['job_details'] = None
                result['error'] = f"è·å–ä»»åŠ¡è¯¦æƒ…å¤±è´¥: {e}"
        
        app_logger.info(f"âœ… è¯„è®ºå¤„ç†çŠ¶æ€ç»Ÿè®¡è·å–å®Œæˆ: {result}")
        return result
        
    except Exception as e:
        app_logger.error(f"âŒ è·å–è¯„è®ºå¤„ç†çŠ¶æ€å¤±è´¥: {e}")
        return {
            'status': 'failed',
            'error': str(e),
            'message': f'è·å–è¯„è®ºå¤„ç†çŠ¶æ€å¤±è´¥: {e}'
        }