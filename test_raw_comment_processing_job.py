#!/usr/bin/env python3
"""
æµ‹è¯•åŸå§‹è¯„è®ºæ›´æ–°åŠŸèƒ½ä¸processing_jobsè¡¨çš„é›†æˆ
"""
import asyncio
import httpx
import time
from datetime import datetime

# æµ‹è¯•é…ç½®
BASE_URL = "http://localhost:8000"
TEST_CHANNEL_ID = 1
TEST_IDENTIFIER = "s7855"  # ä½¿ç”¨ä¸€ä¸ªå­˜åœ¨çš„è½¦å‹æ ‡è¯†

async def test_raw_comment_processing_job():
    """æµ‹è¯•åŸå§‹è¯„è®ºæ›´æ–°çš„processing_jobé›†æˆ"""
    
    async with httpx.AsyncClient() as client:
        print("ğŸš€ å¼€å§‹æµ‹è¯•åŸå§‹è¯„è®ºæ›´æ–°çš„processing_jobé›†æˆ")
        print(f"æµ‹è¯•è½¦å‹: channel_id={TEST_CHANNEL_ID}, identifier={TEST_IDENTIFIER}")
        print("-" * 60)
        
        # 1. å¯åŠ¨å¼‚æ­¥çˆ¬å–ä»»åŠ¡
        print("1ï¸âƒ£ å¯åŠ¨åŸå§‹è¯„è®ºå¼‚æ­¥çˆ¬å–ä»»åŠ¡...")
        crawl_request = {
            "channel_id": TEST_CHANNEL_ID,
            "identifier_on_channel": TEST_IDENTIFIER,
            "max_pages": 2  # é™åˆ¶é¡µæ•°ä»¥åŠ å¿«æµ‹è¯•
        }
        
        try:
            response = await client.post(
                f"{BASE_URL}/api/raw-comments/crawl",
                json=crawl_request
            )
            response.raise_for_status()
            task_info = response.json()
            
            print(f"âœ… ä»»åŠ¡å¯åŠ¨æˆåŠŸ:")
            print(f"   Task ID: {task_info['task_id']}")
            print(f"   Job ID: {task_info['job_id']}")
            print(f"   Status: {task_info['status']}")
            print(f"   Message: {task_info['message']}")
            
            task_id = task_info['task_id']
            job_id = task_info['job_id']
            
        except httpx.HTTPError as e:
            print(f"âŒ å¯åŠ¨ä»»åŠ¡å¤±è´¥: {e}")
            return
        
        # 2. ç«‹å³æŸ¥è¯¢processing_jobçŠ¶æ€
        print(f"\n2ï¸âƒ£ æŸ¥è¯¢processing_jobåˆå§‹çŠ¶æ€...")
        try:
            response = await client.get(f"{BASE_URL}/api/raw-comments/jobs/{job_id}")
            response.raise_for_status()
            job_status = response.json()
            
            print(f"âœ… ProcessingJobçŠ¶æ€:")
            print(f"   Job ID: {job_status['job_id']}")
            print(f"   Task Type: {job_status['task_type']}")
            print(f"   Status: {job_status['status']}")
            print(f"   Pipeline Version: {job_status['pipeline_version']}")
            print(f"   Created At: {job_status['created_at']}")
            print(f"   Task Params: {job_status['task_params']}")
            
        except httpx.HTTPError as e:
            print(f"âŒ æŸ¥è¯¢processing_jobå¤±è´¥: {e}")
        
        # 3. ç›‘æ§ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹
        print(f"\n3ï¸âƒ£ ç›‘æ§ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹...")
        max_wait_time = 120  # æœ€å¤§ç­‰å¾…2åˆ†é’Ÿ
        start_time = time.time()
        
        while time.time() - start_time < max_wait_time:
            try:
                # æŸ¥è¯¢Celeryä»»åŠ¡çŠ¶æ€
                response = await client.get(f"{BASE_URL}/api/raw-comments/tasks/{task_id}/status")
                response.raise_for_status()
                celery_status = response.json()
                
                # æŸ¥è¯¢ProcessingJobçŠ¶æ€
                response = await client.get(f"{BASE_URL}/api/raw-comments/jobs/{job_id}")
                response.raise_for_status()
                job_status = response.json()
                
                current_time = datetime.now().strftime("%H:%M:%S")
                print(f"[{current_time}] Celery: {celery_status['status']} | ProcessingJob: {job_status['status']}")
                
                # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                if celery_status['status'] == 'PROGRESS':
                    progress = celery_status.get('progress', 0)
                    print(f"           è¿›åº¦: {progress}%")
                
                if job_status['status'] == 'running' and job_status.get('started_at'):
                    print(f"           å¼€å§‹æ—¶é—´: {job_status['started_at']}")
                
                # ä»»åŠ¡å®Œæˆ
                if celery_status['status'] in ['SUCCESS', 'FAILURE']:
                    print(f"\nğŸ¯ ä»»åŠ¡æ‰§è¡Œå®Œæˆ!")
                    
                    if celery_status['status'] == 'SUCCESS':
                        result = celery_status.get('result', {})
                        print(f"âœ… çˆ¬å–ç»“æœ:")
                        print(f"   è½¦å‹åç§°: {result.get('result', {}).get('vehicle_name', 'N/A')}")
                        print(f"   çˆ¬å–é¡µæ•°: {result.get('result', {}).get('total_pages_crawled', 0)}")
                        print(f"   å‘ç°è¯„è®º: {result.get('result', {}).get('total_comments_found', 0)}")
                        print(f"   æ–°å¢è¯„è®º: {result.get('result', {}).get('new_comments_count', 0)}")
                        print(f"   è€—æ—¶: {result.get('result', {}).get('crawl_duration', 0)}ç§’")
                        
                        # æœ€ç»ˆProcessingJobçŠ¶æ€
                        print(f"\nğŸ“Š æœ€ç»ˆProcessingJobçŠ¶æ€:")
                        print(f"   Status: {job_status['status']}")
                        print(f"   Started At: {job_status.get('started_at', 'N/A')}")
                        print(f"   Completed At: {job_status.get('completed_at', 'N/A')}")
                        print(f"   Result Summary: {job_status.get('result_summary', 'N/A')}")
                        
                    else:
                        print(f"âŒ ä»»åŠ¡å¤±è´¥: {celery_status.get('error', 'Unknown error')}")
                        print(f"   ProcessingJobçŠ¶æ€: {job_status['status']}")
                        print(f"   Result Summary: {job_status.get('result_summary', 'N/A')}")
                    
                    break
                
                await asyncio.sleep(5)  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
                
            except httpx.HTTPError as e:
                print(f"âŒ çŠ¶æ€æŸ¥è¯¢å¤±è´¥: {e}")
                await asyncio.sleep(5)
                continue
        
        else:
            print(f"â° ç­‰å¾…è¶…æ—¶ ({max_wait_time}ç§’)")
        
        # 4. éªŒè¯æ•°æ®åº“çŠ¶æ€
        print(f"\n4ï¸âƒ£ éªŒè¯æ•°æ®åº“çŠ¶æ€...")
        try:
            # æŸ¥è¯¢è½¦å‹è¯„è®ºæ•°é‡
            response = await client.get(f"{BASE_URL}/api/raw-comments/vehicle/{TEST_CHANNEL_ID}/{TEST_IDENTIFIER}/count")
            response.raise_for_status()
            count_info = response.json()
            
            print(f"âœ… è½¦å‹è¯„è®ºç»Ÿè®¡:")
            print(f"   è½¦å‹åç§°: {count_info['vehicle_name']}")
            print(f"   è¯„è®ºæ€»æ•°: {count_info['comment_count']}")
            
        except httpx.HTTPError as e:
            print(f"âŒ ç»Ÿè®¡æŸ¥è¯¢å¤±è´¥: {e}")

if __name__ == "__main__":
    print("ğŸ§ª åŸå§‹è¯„è®ºæ›´æ–°processing_jobé›†æˆæµ‹è¯•")
    print("=" * 60)
    asyncio.run(test_raw_comment_processing_job()) 